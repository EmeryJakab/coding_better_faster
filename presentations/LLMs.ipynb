{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Large Language Models\n",
    "### (And How They Can Help You Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There's Never Been an Easier Time to Write Good Code\n",
    "\n",
    "By using Large Language Models (LLMs), you can more quickly go from code that runs to code you can build on.\n",
    "\n",
    "They can:\n",
    "\n",
    "    * Offer suggestions for code refactoring/organization/functions\n",
    "    * Write you documentation -- both your readme file and function-level documnentation\n",
    "    * Shorten the learning curve for practices like using git for version control\n",
    "    * Write your unit tests and virtual environments\n",
    "\n",
    "You should still expect to be troubleshooting and editing everything these write.\n",
    "\n",
    "**That may change, but it hasn't yet**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You Know What an LLM Is, But We're Going to Define it Anyway\n",
    "\n",
    "A model designed to process and generate human-like text to perform tasks like answering questions, summarizing data, or translating languages. They’re “large” in that they were trained on a lot of data and have a range of capabilities. \n",
    "\n",
    "Examples: \n",
    "* GPT-3.5 and GPT-4 (OpenAI)\n",
    "* Claude and Claude 2 (Anthropic)\n",
    "* Bard (Google)\n",
    "* LLaMA and LLaMA 2 (Facebook/Open Source/many descendants)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A More Technical Definition\n",
    "\n",
    "A large language model (LLM) is a type of **deep learning neural network** (an algorithm inspired by the brain's structure, using layers of computational nodes to recognize patterns in data). Specifically, it's a variant of **transformer architectures** (a cutting-edge structure in machine learning introduced around 2017), which excels in processing language. \n",
    "\n",
    "The LLM operates by handling sequences of tokens (tokens can be words, parts of words, or characters, depending on how the text is split up) and its primary job is to predict subsequent **tokens** (basically guessing the next word in a sequence). \n",
    "\n",
    "These models are massive, having billions of **parameters** (adjustable parts of the model that help it learn from data). A key feature of transformers is their use of **attention mechanisms** (a technique allowing the model to \"focus\" on different parts of the input with varying intensities), which help weigh the importance of different input tokens when generating outputs.\n",
    "\n",
    "*This is the only part of the presentation I just took verbatim from GPT-4.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Is Another Kind of Text Synthesis and Prediction\n",
    "\n",
    "* LLMs are text sythesis and prediction machines, and coding is another kind of text\n",
    "* The LLMs you'll use were trained on A LOT OF CODE -- but how much varies by language\n",
    "* There are code-specific **models** and code-specific **benchmarks** - and you might choose to get deeper into either of those. (Want to make an R benchmark?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There's Also A Lot of Coding-Related Text Online!\n",
    "\n",
    "* LLMs were trained on documentation, too\n",
    "* And lots of guidance on how to do tasks like deal with your git error messages\n",
    "* And, like with code, they can give you more personal (and sometimes even accurate!) advice on those"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "[put something here]\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
